# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a3fCHcZ7xGhSyL7XyACmtBfrYwV9fT4W
"""

!pip install -U langchain langchain-community langchain-core langchain-text-splitters chromadb sentence-transformers gitpython transformers

import os
import shutil
from git import Repo
from transformers import pipeline
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

REPO_PATH = "repo"
CHROMA_PATH = "chroma_db"

# ----------------------------
# Clone GitHub Repository
# ----------------------------
def clone_repo(repo_url):
    if os.path.exists(REPO_PATH):
        shutil.rmtree(REPO_PATH)
    Repo.clone_from(repo_url, REPO_PATH)
    print("Repository cloned successfully ✅")

# ----------------------------
# Load Python Files
# ----------------------------
def load_code_files():
    documents = []
    for root, _, files in os.walk(REPO_PATH):
        for file in files:
            if file.endswith(".py"):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                        documents.append(
                            Document(
                                page_content=f.read(),
                                metadata={"file_name": file}
                            )
                        )
                except:
                    pass
    return documents

# ----------------------------
# Create Vector Database
# ----------------------------
def create_vector_db(documents):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=100
    )

    split_docs = splitter.split_documents(documents)

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    vector_db = Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=CHROMA_PATH
    )

    print("Vector DB created successfully ✅")
    return vector_db

# ----------------------------
# Load LLM
# ----------------------------
def load_llm():
    print("Loading LLM (first time may take 1–2 minutes)...")
    return pipeline(
        "text-generation",
        model="distilgpt2",
        max_new_tokens=120
    )

# ----------------------------
# Ask Question
# ----------------------------
def ask_question(vector_db, model, question):
    docs = vector_db.similarity_search(question, k=3)
    context = "\n\n".join([doc.page_content for doc in docs])

    prompt = f"""
You are a senior software engineer.
Explain clearly how this code works.

Code:
{context}

Question:
{question}

Answer:
"""

    result = model(prompt)[0]["generated_text"]

    # Clean output (remove repeated prompt)
    if "Answer:" in result:
        return result.split("Answer:")[-1].strip()

    return result

# ----------------------------
# Main Program
# ----------------------------
if __name__ == "__main__":
    repo_url = input("Enter GitHub repository URL: ")

    clone_repo(repo_url)

    print("Indexing repository (may take 1–3 minutes)...")
    docs = load_code_files()
    vector_db = create_vector_db(docs)

    model = load_llm()

    print("\nSystem ready! Ask questions about the code.\n")

    while True:
        q = input("Ask: ")
        answer = ask_question(vector_db, model, q)
        print("\nAnswer:\n", answer)